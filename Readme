# Agente de Viagem Virtual com Streamlit e Ollama

Este projeto utiliza o Streamlit para criar uma aplicação de agente de viagem virtual que se comunica com o modelo `llama3` através da API do Ollama.

## Instalação do Ollama

### 1. Instalar Ollama

Para instalar o Ollama, execute o seguinte comando:

```
curl -fsSL https://ollama.com/install.sh | sh
```

### 2. Iniciando servidor Ollama:

```
ollama serve 
```

### 3. Baixando modelo llama3 para usar no projeto:

```
ollama run llama3
```

Obs.: Qualquer problema na instalação do Ollama é possível verificar no site oficial da aplicação: https://ollama.com/


## Instalação e Configuração da Aplicação


1. Clonar o Repositório

Clone este repositório para a sua máquina local:

```
git clone <URL_DO_REPOSITORIO>
cd <DIRETORIO_DO_REPOSITORIO>
```

2. Instalar Dependências

```
python -m venv venv
source venv/bin/activate  # Para Windows: venv\Scripts\activate
pip install -r requirements.txt
```

## Executando a Aplicação

Para iniciar a aplicação Streamlit, execute o seguinte comando:

```
streamlit run agente_de_viagem.py
```